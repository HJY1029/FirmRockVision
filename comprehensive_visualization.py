#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç£çŸ³ä¹‹çœ¼ï¼ˆFirmRock Visionï¼‰- æ™ºèƒ½ç½‘ç»œå…¥ä¾µæ£€æµ‹ä¸å¨èƒåˆ†æç³»ç»Ÿ - ç»¼åˆå¯è§†åŒ–åˆ†æ
åŒ…å«äºŒåˆ†ç±»æ£€æµ‹ã€å¤šåˆ†ç±»è¯†åˆ«ã€æ¨¡å‹å¯¹æ¯”çš„å®Œæ•´å¯è§†åŒ–
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pickle
import warnings
from sklearn.metrics import (
    confusion_matrix, classification_report, roc_curve, 
    roc_auc_score, precision_recall_curve, auc
)

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®å›¾è¡¨æ ·å¼
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 8)


def load_data_and_models():
    """åŠ è½½æ•°æ®å’Œæ¨¡å‹"""
    data_dir = Path('processed_data')
    models_dir = Path('models')
    
    if not data_dir.exists():
        print("é”™è¯¯: æœªæ‰¾åˆ°é¢„å¤„ç†æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ data_preprocessing.py")
        return None, None, None
    
    # åŠ è½½æ•°æ®
    print("åŠ è½½æ•°æ®...")
    X_test = pd.read_csv(data_dir / 'X_test.csv')
    y_test = pd.read_csv(data_dir / 'y_test.csv').values.ravel()
    
    # åŠ è½½æ”»å‡»ç±»å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    attack_cats_test = None
    if (data_dir / 'attack_cat_test.csv').exists():
        attack_cats_test = pd.read_csv(data_dir / 'attack_cat_test.csv').values.ravel()
    
    # åŠ è½½æ¨¡å‹
    models = {}
    if models_dir.exists():
        print("åŠ è½½æ¨¡å‹...")
        for model_file in models_dir.glob('*.pkl'):
            if 'encoder' not in model_file.stem:
                model_name = model_file.stem
                try:
                    with open(model_file, 'rb') as f:
                        models[model_name] = pickle.load(f)
                    print(f"  å·²åŠ è½½: {model_name}")
                except Exception as e:
                    print(f"  è­¦å‘Š: æ— æ³•åŠ è½½ {model_name}: {e}")
    
    return X_test, y_test, attack_cats_test, models


def visualize_binary_classification(X_test, y_test, models):
    """å¯è§†åŒ–äºŒåˆ†ç±»æ£€æµ‹ç»“æœ"""
    print("\n" + "="*80)
    print("ç”ŸæˆäºŒåˆ†ç±»æ£€æµ‹å¯è§†åŒ–")
    print("="*80)
    
    binary_models = {k: v for k, v in models.items() if 'binary' in k}
    
    if len(binary_models) == 0:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°äºŒåˆ†ç±»æ¨¡å‹")
        return
    
    n_models = len(binary_models)
    fig = plt.figure(figsize=(20, 12))
    gs = fig.add_gridspec(3, n_models, hspace=0.3, wspace=0.3)
    
    fig.suptitle('äºŒåˆ†ç±»æ£€æµ‹ç»“æœ - æ­£å¸¸ vs æ”»å‡»', fontsize=18, fontweight='bold', y=0.98)
    
    model_names = []
    accuracies = []
    precisions = []
    recalls = []
    f1_scores = []
    roc_aucs = []
    
    for idx, (model_name, model) in enumerate(binary_models.items()):
        display_name = model_name.replace('_binary', '')
        model_names.append(display_name)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        y_pred_proba = None
        if hasattr(model, 'predict_proba'):
            y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # è®¡ç®—æŒ‡æ ‡
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred, zero_division=0)
        rec = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        accuracies.append(acc)
        precisions.append(prec)
        recalls.append(rec)
        f1_scores.append(f1)
        
        # ROC AUC
        roc_auc = None
        if y_pred_proba is not None:
            try:
                roc_auc = roc_auc_score(y_test, y_pred_proba)
                roc_aucs.append(roc_auc)
            except:
                roc_aucs.append(None)
        else:
            roc_aucs.append(None)
        
        # 1. æ··æ·†çŸ©é˜µ
        ax1 = fig.add_subplot(gs[0, idx])
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
                   xticklabels=['æ­£å¸¸', 'æ”»å‡»'],
                   yticklabels=['æ­£å¸¸', 'æ”»å‡»'],
                   cbar_kws={'label': 'æ ·æœ¬æ•°'})
        ax1.set_title(f'{display_name}\nå‡†ç¡®ç‡: {acc:.4f}', fontsize=12, fontweight='bold')
        ax1.set_ylabel('å®é™…æ ‡ç­¾', fontsize=10)
        ax1.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=10)
        
        # 2. ROCæ›²çº¿
        ax2 = fig.add_subplot(gs[1, idx])
        if y_pred_proba is not None:
            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
            ax2.plot(fpr, tpr, linewidth=2, label=f'AUC = {roc_auc:.4f}')
            ax2.plot([0, 1], [0, 1], 'k--', linewidth=1, label='éšæœºçŒœæµ‹')
            ax2.set_xlim([0.0, 1.0])
            ax2.set_ylim([0.0, 1.05])
            ax2.set_xlabel('å‡é˜³æ€§ç‡ (FPR)', fontsize=10)
            ax2.set_ylabel('çœŸé˜³æ€§ç‡ (TPR)', fontsize=10)
            ax2.set_title('ROCæ›²çº¿', fontsize=11, fontweight='bold')
            ax2.legend(loc="lower right", fontsize=9)
            ax2.grid(alpha=0.3)
        else:
            ax2.text(0.5, 0.5, 'æ— æ¦‚ç‡é¢„æµ‹', ha='center', va='center', fontsize=12)
            ax2.set_title('ROCæ›²çº¿', fontsize=11, fontweight='bold')
        
        # 3. ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿
        ax3 = fig.add_subplot(gs[2, idx])
        if y_pred_proba is not None:
            precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
            pr_auc = auc(recall, precision)
            ax3.plot(recall, precision, linewidth=2, label=f'PR-AUC = {pr_auc:.4f}')
            ax3.set_xlim([0.0, 1.0])
            ax3.set_ylim([0.0, 1.05])
            ax3.set_xlabel('å¬å›ç‡ (Recall)', fontsize=10)
            ax3.set_ylabel('ç²¾ç¡®ç‡ (Precision)', fontsize=10)
            ax3.set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿', fontsize=11, fontweight='bold')
            ax3.legend(loc="lower left", fontsize=9)
            ax3.grid(alpha=0.3)
        else:
            ax3.text(0.5, 0.5, 'æ— æ¦‚ç‡é¢„æµ‹', ha='center', va='center', fontsize=12)
            ax3.set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿', fontsize=11, fontweight='bold')
    
    # ä¿å­˜å›¾è¡¨
    output_path = Path('results') / '01_äºŒåˆ†ç±»æ£€æµ‹ç»“æœ.png'
    Path('results').mkdir(exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"  å·²ä¿å­˜: {output_path}")
    plt.close()
    
    # ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æŸ±çŠ¶å›¾
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))
    fig.suptitle('äºŒåˆ†ç±»æ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
    
    metrics_data = {
        'å‡†ç¡®ç‡': accuracies,
        'ç²¾ç¡®ç‡': precisions,
        'å¬å›ç‡': recalls,
        'F1åˆ†æ•°': f1_scores
    }
    
    for idx, (metric_name, values) in enumerate(metrics_data.items()):
        ax = axes[idx // 2, idx % 2]
        bars = ax.bar(range(len(model_names)), values, 
                     color=sns.color_palette("husl", len(model_names)))
        ax.set_xticks(range(len(model_names)))
        ax.set_xticklabels(model_names, rotation=45, ha='right')
        ax.set_ylabel(metric_name, fontsize=12)
        ax.set_title(f'{metric_name} å¯¹æ¯”', fontsize=13, fontweight='bold')
        ax.set_ylim([0, 1.1])
        ax.grid(axis='y', alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, val in zip(bars, values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                   f'{val:.4f}', ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    output_path = Path('results') / '02_äºŒåˆ†ç±»æ¨¡å‹æ€§èƒ½å¯¹æ¯”.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"  å·²ä¿å­˜: {output_path}")
    plt.close()
    
    return {
        'model_names': model_names,
        'accuracies': accuracies,
        'precisions': precisions,
        'recalls': recalls,
        'f1_scores': f1_scores,
        'roc_aucs': roc_aucs
    }


def visualize_multiclass_classification(X_test, y_test, attack_cats_test, models):
    """å¯è§†åŒ–å¤šåˆ†ç±»è¯†åˆ«ç»“æœï¼ˆæ”»å‡»ç±»å‹åˆ†ç±»ï¼‰"""
    print("\n" + "="*80)
    print("ç”Ÿæˆå¤šåˆ†ç±»è¯†åˆ«å¯è§†åŒ–")
    print("="*80)
    
    if attack_cats_test is None:
        print("è­¦å‘Š: æœªæ‰¾åˆ°æ”»å‡»ç±»å‹æ•°æ®ï¼Œè·³è¿‡å¤šåˆ†ç±»å¯è§†åŒ–")
        return None
    
    # åªä½¿ç”¨æ”»å‡»æ ·æœ¬
    attack_mask = y_test == 1
    if attack_mask.sum() == 0:
        print("è­¦å‘Š: æµ‹è¯•é›†ä¸­æ²¡æœ‰æ”»å‡»æ ·æœ¬")
        return None
    
    X_test_attack = X_test[attack_mask]
    attack_cats_test_filtered = attack_cats_test[attack_mask]
    
    # æ‰¾åˆ°å¤šåˆ†ç±»æ¨¡å‹
    multi_models = {}
    encoders = {}
    
    for model_name, model in models.items():
        if 'Multi' in model_name or 'multi' in model_name:
            if 'encoder' not in model_name:
                multi_models[model_name] = model
                # æŸ¥æ‰¾å¯¹åº”çš„ç¼–ç å™¨
                encoder_name = f'{model_name}_encoder'
                if encoder_name in models:
                    encoders[model_name] = models[encoder_name]
    
    if len(multi_models) == 0:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°å¤šåˆ†ç±»æ¨¡å‹")
        return None
    
    # ç¼–ç æ”»å‡»ç±»å‹
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_test_encoded = le.fit_transform(attack_cats_test_filtered)
    attack_types = le.classes_
    
    print(f"\næ”»å‡»ç±»å‹ ({len(attack_types)} ç§):")
    for i, atype in enumerate(attack_types):
        count = (attack_cats_test_filtered == atype).sum()
        print(f"  {i+1}. {atype:<20} æ ·æœ¬æ•°: {count:>6,}")
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆå¯è§†åŒ–
    for model_name, model in multi_models.items():
        display_name = model_name.replace('_Multi', '').replace('_multi', '')
        
        # é¢„æµ‹
        y_pred_encoded = model.predict(X_test_attack)
        y_pred = le.inverse_transform(y_pred_encoded)
        
        # è®¡ç®—æŒ‡æ ‡
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        acc = accuracy_score(y_test_encoded, y_pred_encoded)
        prec = precision_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)
        rec = recall_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)
        f1 = f1_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)
        
        # åˆ›å»ºå¯è§†åŒ–
        fig, axes = plt.subplots(2, 2, figsize=(18, 14))
        fig.suptitle(f'å¤šåˆ†ç±»è¯†åˆ«ç»“æœ - {display_name}\næ”»å‡»ç±»å‹åˆ†ç±»', 
                     fontsize=16, fontweight='bold')
        
        # 1. æ··æ·†çŸ©é˜µ
        ax1 = axes[0, 0]
        cm = confusion_matrix(y_test_encoded, y_pred_encoded)
        sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', ax=ax1,
                   xticklabels=attack_types, yticklabels=attack_types,
                   cbar_kws={'label': 'æ ·æœ¬æ•°'})
        ax1.set_title(f'æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {acc:.4f}', fontsize=12, fontweight='bold')
        ax1.set_ylabel('å®é™…æ”»å‡»ç±»å‹', fontsize=11)
        ax1.set_xlabel('é¢„æµ‹æ”»å‡»ç±»å‹', fontsize=11)
        plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')
        plt.setp(ax1.get_yticklabels(), rotation=0)
        
        # 2. æ¯ä¸ªæ”»å‡»ç±»å‹çš„æ€§èƒ½æŒ‡æ ‡
        ax2 = axes[0, 1]
        from sklearn.metrics import precision_recall_fscore_support
        precisions_per_class, recalls_per_class, f1s_per_class, _ = \
            precision_recall_fscore_support(y_test_encoded, y_pred_encoded, 
                                          zero_division=0, labels=range(len(attack_types)))
        
        x = np.arange(len(attack_types))
        width = 0.25
        
        ax2.bar(x - width, precisions_per_class, width, label='ç²¾ç¡®ç‡', alpha=0.8)
        ax2.bar(x, recalls_per_class, width, label='å¬å›ç‡', alpha=0.8)
        ax2.bar(x + width, f1s_per_class, width, label='F1åˆ†æ•°', alpha=0.8)
        
        ax2.set_xlabel('æ”»å‡»ç±»å‹', fontsize=11)
        ax2.set_ylabel('åˆ†æ•°', fontsize=11)
        ax2.set_title('å„ç±»æ”»å‡»ç±»å‹æ€§èƒ½æŒ‡æ ‡', fontsize=12, fontweight='bold')
        ax2.set_xticks(x)
        ax2.set_xticklabels(attack_types, rotation=45, ha='right')
        ax2.set_ylim([0, 1.1])
        ax2.legend(fontsize=10)
        ax2.grid(axis='y', alpha=0.3)
        
        # 3. æ”»å‡»ç±»å‹åˆ†å¸ƒï¼ˆå®é™… vs é¢„æµ‹ï¼‰
        ax3 = axes[1, 0]
        actual_counts = pd.Series(attack_cats_test_filtered).value_counts().sort_index()
        pred_counts = pd.Series(y_pred).value_counts().sort_index()
        
        x = np.arange(len(attack_types))
        width = 0.35
        
        ax3.bar(x - width/2, [actual_counts.get(at, 0) for at in attack_types], 
               width, label='å®é™…', alpha=0.8)
        ax3.bar(x + width/2, [pred_counts.get(at, 0) for at in attack_types], 
               width, label='é¢„æµ‹', alpha=0.8)
        
        ax3.set_xlabel('æ”»å‡»ç±»å‹', fontsize=11)
        ax3.set_ylabel('æ ·æœ¬æ•°', fontsize=11)
        ax3.set_title('æ”»å‡»ç±»å‹åˆ†å¸ƒå¯¹æ¯”', fontsize=12, fontweight='bold')
        ax3.set_xticks(x)
        ax3.set_xticklabels(attack_types, rotation=45, ha='right')
        ax3.legend(fontsize=10)
        ax3.grid(axis='y', alpha=0.3)
        
        # 4. åˆ†ç±»æŠ¥å‘Šçƒ­åŠ›å›¾
        ax4 = axes[1, 1]
        report = classification_report(y_test_encoded, y_pred_encoded,
                                      target_names=attack_types,
                                      output_dict=True, zero_division=0)
        
        # æå–æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        metrics_data = []
        for atype in attack_types:
            if atype in report:
                metrics_data.append([
                    report[atype]['precision'],
                    report[atype]['recall'],
                    report[atype]['f1-score'],
                    report[atype]['support']
                ])
            else:
                metrics_data.append([0, 0, 0, 0])
        
        metrics_df = pd.DataFrame(metrics_data, 
                                 index=attack_types,
                                 columns=['ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°', 'æ ·æœ¬æ•°'])
        
        # å½’ä¸€åŒ–æ ·æœ¬æ•°ç”¨äºæ˜¾ç¤º
        metrics_df_display = metrics_df.copy()
        metrics_df_display['æ ·æœ¬æ•°'] = metrics_df_display['æ ·æœ¬æ•°'] / metrics_df_display['æ ·æœ¬æ•°'].max()
        
        sns.heatmap(metrics_df_display, annot=True, fmt='.3f', cmap='RdYlGn', 
                   ax=ax4, cbar_kws={'label': 'å½’ä¸€åŒ–åˆ†æ•°'})
        ax4.set_title('åˆ†ç±»æŠ¥å‘Šçƒ­åŠ›å›¾', fontsize=12, fontweight='bold')
        ax4.set_ylabel('æ”»å‡»ç±»å‹', fontsize=11)
        plt.setp(ax4.get_xticklabels(), rotation=0)
        plt.setp(ax4.get_yticklabels(), rotation=0)
        
        plt.tight_layout()
        output_path = Path('results') / f'03_å¤šåˆ†ç±»è¯†åˆ«ç»“æœ_{display_name}.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"  å·²ä¿å­˜: {output_path}")
        plt.close()
    
    return {
        'attack_types': attack_types,
        'model_performance': {}
    }


def visualize_model_comparison(binary_results, multi_results):
    """å¯è§†åŒ–æ¨¡å‹å¯¹æ¯”"""
    print("\n" + "="*80)
    print("ç”Ÿæˆæ¨¡å‹å¯¹æ¯”å¯è§†åŒ–")
    print("="*80)
    
    if binary_results is None:
        print("è­¦å‘Š: æ²¡æœ‰äºŒåˆ†ç±»ç»“æœ")
        return
    
    # åˆ›å»ºç»¼åˆå¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 2, figsize=(18, 12))
    fig.suptitle('æ¨¡å‹ç»¼åˆæ€§èƒ½å¯¹æ¯”', fontsize=18, fontweight='bold')
    
    model_names = binary_results['model_names']
    
    # 1. æ‰€æœ‰æŒ‡æ ‡å¯¹æ¯”ï¼ˆé›·è¾¾å›¾é£æ ¼ï¼‰
    ax1 = axes[0, 0]
    metrics = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°']
    values = [
        binary_results['accuracies'],
        binary_results['precisions'],
        binary_results['recalls'],
        binary_results['f1_scores']
    ]
    
    x = np.arange(len(metrics))
    width = 0.2
    
    for i, (name, vals) in enumerate(zip(model_names, zip(*values))):
        offset = (i - len(model_names)/2 + 0.5) * width
        ax1.bar(x + offset, vals, width, label=name, alpha=0.8)
    
    ax1.set_xlabel('è¯„ä¼°æŒ‡æ ‡', fontsize=12)
    ax1.set_ylabel('åˆ†æ•°', fontsize=12)
    ax1.set_title('æ‰€æœ‰æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”', fontsize=13, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(metrics)
    ax1.set_ylim([0, 1.1])
    ax1.legend(fontsize=10)
    ax1.grid(axis='y', alpha=0.3)
    
    # 2. ROC AUCå¯¹æ¯”ï¼ˆå¦‚æœæœ‰ï¼‰
    ax2 = axes[0, 1]
    roc_aucs = [r for r in binary_results['roc_aucs'] if r is not None]
    roc_names = [n for n, r in zip(model_names, binary_results['roc_aucs']) if r is not None]
    
    if len(roc_aucs) > 0:
        bars = ax2.bar(range(len(roc_names)), roc_aucs, 
                      color=sns.color_palette("husl", len(roc_names)))
        ax2.set_xticks(range(len(roc_names)))
        ax2.set_xticklabels(roc_names, rotation=45, ha='right')
        ax2.set_ylabel('ROC AUC', fontsize=12)
        ax2.set_title('ROC AUC å¯¹æ¯”', fontsize=13, fontweight='bold')
        ax2.set_ylim([0, 1.1])
        ax2.grid(axis='y', alpha=0.3)
        
        for bar, val in zip(bars, roc_aucs):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{val:.4f}', ha='center', va='bottom', fontsize=10)
    else:
        ax2.text(0.5, 0.5, 'æ— ROC AUCæ•°æ®', ha='center', va='center', fontsize=12)
        ax2.set_title('ROC AUC å¯¹æ¯”', fontsize=13, fontweight='bold')
    
    # 3. ç»¼åˆè¯„åˆ†ï¼ˆå¹³å‡æ‰€æœ‰æŒ‡æ ‡ï¼‰
    ax3 = axes[1, 0]
    overall_scores = []
    for i in range(len(model_names)):
        score = (binary_results['accuracies'][i] + 
                binary_results['precisions'][i] + 
                binary_results['recalls'][i] + 
                binary_results['f1_scores'][i]) / 4
        overall_scores.append(score)
    
    bars = ax3.bar(range(len(model_names)), overall_scores,
                  color=sns.color_palette("husl", len(model_names)))
    ax3.set_xticks(range(len(model_names)))
    ax3.set_xticklabels(model_names, rotation=45, ha='right')
    ax3.set_ylabel('ç»¼åˆè¯„åˆ†', fontsize=12)
    ax3.set_title('æ¨¡å‹ç»¼åˆè¯„åˆ†ï¼ˆå¹³å‡æ‰€æœ‰æŒ‡æ ‡ï¼‰', fontsize=13, fontweight='bold')
    ax3.set_ylim([0, 1.1])
    ax3.grid(axis='y', alpha=0.3)
    
    for bar, val in zip(bars, overall_scores):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{val:.4f}', ha='center', va='bottom', fontsize=10)
    
    # 4. æ€§èƒ½æŒ‡æ ‡è¡¨æ ¼
    ax4 = axes[1, 1]
    ax4.axis('tight')
    ax4.axis('off')
    
    table_data = []
    for i, name in enumerate(model_names):
        row = [
            name,
            f"{binary_results['accuracies'][i]:.4f}",
            f"{binary_results['precisions'][i]:.4f}",
            f"{binary_results['recalls'][i]:.4f}",
            f"{binary_results['f1_scores'][i]:.4f}"
        ]
        if binary_results['roc_aucs'][i] is not None:
            row.append(f"{binary_results['roc_aucs'][i]:.4f}")
        else:
            row.append("N/A")
        table_data.append(row)
    
    columns = ['æ¨¡å‹', 'å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°', 'ROC AUC']
    table = ax4.table(cellText=table_data, colLabels=columns,
                     cellLoc='center', loc='center',
                     colWidths=[0.2, 0.15, 0.15, 0.15, 0.15, 0.15])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)
    
    # è®¾ç½®è¡¨å¤´æ ·å¼
    for i in range(len(columns)):
        table[(0, i)].set_facecolor('#4CAF50')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    ax4.set_title('æ¨¡å‹æ€§èƒ½æŒ‡æ ‡æ±‡æ€»è¡¨', fontsize=13, fontweight='bold', pad=20)
    
    plt.tight_layout()
    output_path = Path('results') / '04_æ¨¡å‹ç»¼åˆå¯¹æ¯”.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"  å·²ä¿å­˜: {output_path}")
    plt.close()


def create_data_explanation():
    """åˆ›å»ºæ•°æ®æ–‡ä»¶è¯´æ˜å›¾"""
    print("\n" + "="*80)
    print("ç”Ÿæˆæ•°æ®æ–‡ä»¶è¯´æ˜")
    print("="*80)
    
    fig, ax = plt.subplots(figsize=(16, 10))
    ax.axis('off')
    
    explanation_text = """
    æ•°æ®æ–‡ä»¶è¯´æ˜
    
    ğŸ“ processed_data/ ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼š
    
    1. X_train.csv / X_test.csv
       - å«ä¹‰ï¼šç‰¹å¾æ•°æ®ï¼ˆFeaturesï¼‰
       - å†…å®¹ï¼š30ä¸ªç»è¿‡é€‰æ‹©å’Œé¢„å¤„ç†çš„ç‰¹å¾åˆ—
       - ç”¨é€”ï¼šè¾“å…¥åˆ°æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œè®­ç»ƒ/é¢„æµ‹
       - ç¤ºä¾‹ï¼šsttl, sbytes, ct_state_ttl, sload, smean ç­‰
    
    2. y_train.csv / y_test.csv
       - å«ä¹‰ï¼šæ ‡ç­¾æ•°æ®ï¼ˆLabelsï¼‰
       - å†…å®¹ï¼šäºŒåˆ†ç±»æ ‡ç­¾ï¼ˆ0=æ­£å¸¸æµé‡ï¼Œ1=æ”»å‡»æµé‡ï¼‰
       - ç”¨é€”ï¼šç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
       - ç¤ºä¾‹ï¼š0, 0, 1, 0, 1, ...
    
    3. attack_cat_train.csv / attack_cat_test.csv
       - å«ä¹‰ï¼šæ”»å‡»ç±»å‹æ ‡ç­¾ï¼ˆAttack Categoriesï¼‰
       - å†…å®¹ï¼š9ç§æ”»å‡»ç±»å‹çš„åç§°
       - ç”¨é€”ï¼šç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ï¼ˆè¯†åˆ«å…·ä½“æ”»å‡»ç±»å‹ï¼‰
       - ç¤ºä¾‹ï¼šNormal, Fuzzers, Analysis, Backdoors, DoS, Exploits, ...
    
    4. preprocessor.pkl
       - å«ä¹‰ï¼šæ•°æ®é¢„å¤„ç†å™¨
       - å†…å®¹ï¼šä¿å­˜çš„ç‰¹å¾ç¼–ç å™¨å’Œæ ‡å‡†åŒ–å™¨
       - ç”¨é€”ï¼šå¯¹æ–°æ•°æ®è¿›è¡Œç›¸åŒçš„é¢„å¤„ç†
    
    ğŸ“Š æ•°æ®æµç¨‹ï¼š
    
    åŸå§‹æ•°æ® â†’ ç‰¹å¾é€‰æ‹© â†’ æ•°æ®é¢„å¤„ç† â†’ æ¨¡å‹è®­ç»ƒ â†’ æ¨¡å‹è¯„ä¼°
    
    UNSW_NB15_training-set.csv  â†’  X_train.csv + y_train.csv
    UNSW_NB15_testing-set.csv   â†’  X_test.csv + y_test.csv
    
    """
    
    ax.text(0.5, 0.5, explanation_text, 
           transform=ax.transAxes,
           fontsize=14,
           verticalalignment='center',
           horizontalalignment='center',
           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
           family='monospace')
    
    ax.set_title('æ•°æ®æ–‡ä»¶è¯´æ˜', fontsize=18, fontweight='bold', pad=20)
    
    output_path = Path('results') / '00_æ•°æ®æ–‡ä»¶è¯´æ˜.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"  å·²ä¿å­˜: {output_path}")
    plt.close()


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print(" " * 10 + "ç£çŸ³ä¹‹çœ¼ï¼ˆFirmRock Visionï¼‰- ç»¼åˆå¯è§†åŒ–åˆ†æ")
    print("="*80)
    
    # åˆ›å»ºç»“æœç›®å½•
    Path('results').mkdir(exist_ok=True)
    
    # åˆ›å»ºæ•°æ®æ–‡ä»¶è¯´æ˜
    create_data_explanation()
    
    # åŠ è½½æ•°æ®å’Œæ¨¡å‹
    result = load_data_and_models()
    if result is None:
        return
    
    X_test, y_test, attack_cats_test, models = result
    
    if len(models) == 0:
        print("é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œ train_models.py")
        return
    
    # 1. äºŒåˆ†ç±»æ£€æµ‹å¯è§†åŒ–
    binary_results = visualize_binary_classification(X_test, y_test, models)
    
    # 2. å¤šåˆ†ç±»è¯†åˆ«å¯è§†åŒ–
    multi_results = visualize_multiclass_classification(X_test, y_test, attack_cats_test, models)
    
    # 3. æ¨¡å‹å¯¹æ¯”å¯è§†åŒ–
    visualize_model_comparison(binary_results, multi_results)
    
    print("\n" + "="*80)
    print("ç»¼åˆå¯è§†åŒ–åˆ†æå®Œæˆ!")
    print("="*80)
    print(f"\næ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {Path('results').absolute()}")
    print("\nç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶ï¼š")
    print("  00_æ•°æ®æ–‡ä»¶è¯´æ˜.png - æ•°æ®æ–‡ä»¶è¯´æ˜")
    print("  01_äºŒåˆ†ç±»æ£€æµ‹ç»“æœ.png - äºŒåˆ†ç±»æ£€æµ‹è¯¦ç»†ç»“æœ")
    print("  02_äºŒåˆ†ç±»æ¨¡å‹æ€§èƒ½å¯¹æ¯”.png - äºŒåˆ†ç±»æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("  03_å¤šåˆ†ç±»è¯†åˆ«ç»“æœ_*.png - å¤šåˆ†ç±»è¯†åˆ«è¯¦ç»†ç»“æœï¼ˆæ¯ä¸ªæ¨¡å‹ä¸€å¼ ï¼‰")
    print("  04_æ¨¡å‹ç»¼åˆå¯¹æ¯”.png - æ¨¡å‹ç»¼åˆæ€§èƒ½å¯¹æ¯”")


if __name__ == "__main__":
    main()

